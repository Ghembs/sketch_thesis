\babel@toc {italian}{}
\babel@toc {italian}{}
\deactivateaddvspace 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1}{\ignorespaces Interpolazioni nello spazio di latenza di immagini vettoriali generate dal modello.\relax }}{7}{figure.caption.3}
\contentsline {figure}{\numberline {2}{\ignorespaces Rappresentazione di un neurone artificiale.\relax }}{9}{figure.caption.4}
\contentsline {figure}{\numberline {3}{\ignorespaces Rappresentazione di un modello Multi-Strato.\relax }}{10}{figure.caption.5}
\contentsline {figure}{\numberline {4}{\ignorespaces Una semplice RNN (Recurrent Neural Network).\relax }}{10}{figure.caption.6}
\contentsline {figure}{\numberline {5}{\ignorespaces Una RNN dispiegata lungo la linea temporale.\relax }}{11}{figure.caption.7}
\contentsline {figure}{\numberline {6}{\ignorespaces Due diversi metodi di implementazione della memoria in una RNN\relax }}{12}{figure.caption.8}
\contentsline {figure}{\numberline {7}{\ignorespaces Combinazioni di sequenze vettoriali. \cite {rnn_effect}\relax }}{13}{figure.caption.9}
\contentsline {figure}{\numberline {8}{\ignorespaces Struttura interna di una RNN standard con un singolo hidden layer.\relax }}{14}{figure.caption.10}
\contentsline {figure}{\numberline {9}{\ignorespaces Struttura interna di una lstm che evidenzia i quattro strati interni di un layer\relax }}{14}{figure.caption.11}
\contentsline {figure}{\numberline {10}{\ignorespaces Confronto sull'utilizzo dell'input in diverse reti neurali.\relax }}{16}{figure.caption.12}
\contentsline {figure}{\numberline {11}{\ignorespaces Struttura generica di una BRNN, svolta lungo tre time steps.\relax }}{17}{figure.caption.13}
\contentsline {figure}{\numberline {12}{\ignorespaces Struttura di un generico AutoEncoder, da \cite {keras_blog}\relax }}{18}{figure.caption.14}
\contentsline {figure}{\numberline {13}{\ignorespaces Modello di un VAE\relax }}{21}{figure.caption.15}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
