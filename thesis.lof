\babel@toc {italian}{}
\babel@toc {italian}{}
\deactivateaddvspace 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1}{\ignorespaces Interpolazioni nello spazio di latenza di immagini vettoriali generate dal modello.\relax }}{7}{figure.caption.3}
\contentsline {figure}{\numberline {2}{\ignorespaces Rappresentazione di un neurone artificiale.\relax }}{9}{figure.caption.4}
\contentsline {figure}{\numberline {3}{\ignorespaces Rappresentazione di un modello Multi-Strato.\relax }}{10}{figure.caption.5}
\contentsline {figure}{\numberline {4}{\ignorespaces Una semplice RNN (Recurrent Neural Network).\relax }}{10}{figure.caption.6}
\contentsline {figure}{\numberline {5}{\ignorespaces Una RNN dispiegata lungo la linea temporale.\relax }}{11}{figure.caption.7}
\contentsline {figure}{\numberline {6}{\ignorespaces Due diversi metodi di implementazione della memoria in una RNN\relax }}{12}{figure.caption.8}
\contentsline {figure}{\numberline {7}{\ignorespaces Combinazioni di sequenze vettoriali. \cite {rnn_effect}\relax }}{13}{figure.caption.9}
\contentsline {figure}{\numberline {8}{\ignorespaces Struttura interna di una RNN standard con un singolo hidden layer.\relax }}{14}{figure.caption.10}
\contentsline {figure}{\numberline {9}{\ignorespaces Struttura interna di una lstm che evidenzia i quattro strati interni di un layer\relax }}{14}{figure.caption.11}
\contentsline {figure}{\numberline {10}{\ignorespaces Confronto sull'utilizzo dell'input in diverse reti neurali.\relax }}{16}{figure.caption.12}
\contentsline {figure}{\numberline {11}{\ignorespaces Struttura generica di una BRNN, svolta lungo tre time steps.\relax }}{17}{figure.caption.13}
\contentsline {figure}{\numberline {12}{\ignorespaces Struttura di un generico AutoEncoder, da \cite {keras_blog}\relax }}{18}{figure.caption.14}
\contentsline {figure}{\numberline {13}{\ignorespaces Modello di un VAE, da \cite {VAE_tut}\relax }}{21}{figure.caption.15}
\contentsline {figure}{\numberline {14}{\ignorespaces Un VAE per intero, a sinistra senza il "reparametrization trick", a destra con. In rosso le operazioni non differenziabili, in blu le loss. Queste due varianti differiscono per il fatto che la retropropagazione si pu\IeC {\`o} applicare solo sulla rete a destra.\relax }}{23}{figure.caption.16}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {15}{\ignorespaces Sketch-rnn\relax }}{26}{figure.caption.17}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {16}{\ignorespaces I layer ottenuti attraverso la configurazione non condizionata.\relax }}{36}{figure.caption.19}
\contentsline {figure}{\numberline {17}{\ignorespaces La prima immagine \IeC {\`e} estratta dal dataset, le altre sono immagini originali generate dal modello standard\relax }}{37}{figure.caption.20}
\contentsline {figure}{\numberline {18}{\ignorespaces Immagini generate dal modello autoregressivo al variare del parametro di temperatura\relax }}{38}{figure.caption.21}
\addvspace {10\p@ }
