\myChapter{Esperimenti}
\section{Introduzione} % (fold)
\label{sec:introduzione}
In questo elaborato si è scelto di riprodurre ed analizzare i risultati presentati da Ha e Eck attraverso due esperimenti: nel primo è stata scelta la configurazione standard del modello, con un VAE completo e layer LSTM sia nell'encoder che nel decoder, la rete è stata addestrata su due dataset: \textit{cat.npz} (gatti) e \textit{flying\_saucer.npz} (dischi volanti). Nel secondo è stata scelta la configurazione con il solo decoder, utilizzato come modello autoregressivo non condizionato ad una variabile latente (con i pesi inizializzati a zero), in questo caso il layer utilizzato è HyperLSTM\footnote{Questo layer non è stato approfondito nell'introduzione di questo elaborato, si rimanda a \cite{hyperlstm} per ulteriori informazioni.}, che eccelle nella generazione di sequenze, il dataset è stato addestrato sul solo dataset \textit{owl.npz} (gufi). Di seguito verranno illustrate e confrontate le reti neurali prodotte da queste configurazioni, inoltre verranno mostrate alcune immagini generate tramite diversi approcci che mostreranno la capacità del modello di concettualizzare le proprietà di un disegno.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language = Python, frame = single, caption = {Iperparametri standard di sketch-rnn}, label = {iperparametri} captionpos = b]
num_steps=10000000,            # Total number of training set. Keep large.
save_every=500,                # Number of batches per checkpoint creation.
dec_rnn_size=512,              # Size of decoder.
dec_model='lstm',              # Decoder: lstm, layer_norm or hyper.
enc_rnn_size=256,              # Size of encoder.
enc_model='lstm',              # Encoder: lstm, layer_norm or hyper.
z_size=128,                    # Size of latent vector z. Recommend 32, 64 or 128.
kl_weight=0.5,                 # KL weight of loss equation. Recommend 0.5 or 1.0.
kl_weight_start=0.01,          # KL start weight when annealing.
kl_tolerance=0.2,              # Level of KL loss at which to stop optimizing for KL.
batch_size=100,                # Minibatch size. Recommend leaving at 100.
grad_clip=1.0,                 # Gradient clipping. Recommend leaving at 1.0.
num_mixture=20,                # Number of mixtures in Gaussian mixture model.
learning_rate=0.001,           # Learning rate.
decay_rate=0.9999,             # Learning rate decay per minibatch.
kl_decay_rate=0.99995,         # KL annealing decay rate per minibatch.
min_learning_rate=0.00001,     # Minimum learning rate.
use_recurrent_dropout=True,    # Recurrent Dropout without Memory Loss. Recomended.
recurrent_dropout_prob=0.90,   # Probability of recurrent dropout keep.
use_input_dropout=False,       # Input dropout. Recommend leaving False.
input_dropout_prob=0.90,       # Probability of input dropout keep.
use_output_dropout=False,      # Output droput. Recommend leaving False.
output_dropout_prob=0.90,      # Probability of output dropout keep.
random_scale_factor=0.15,      # Random scaling data augmention proportion.
augment_stroke_prob=0.10,      # Point dropping augmentation proportion.
conditional=True,              # If False, use decoder-only model.
\end{lstlisting}
\end{minipage}
In questa sezione di codice sono mostrati gli iperparametri del modello, coi loro valori di default. Si può notare che \textit{N\textsubscript{z}} = 128 e che decoder e encoder sono simmetrici\footnote{Si ricordi che l'encoder è un layer bidirezionale, di conseguenza la dimensione dello stato finale, dopo la concatenazione, risulta essere 512.}. A partire da questi valori, viene assemblata la rete neurale mostrata in tabella \ref{tab:1}.
\begin{table}[ht]
	\centering
	\begin{tabular}{ccc}
		\hline
		\hline
		Layer & Output shape & Variabili addestrabili \\
		\hline
		\hline
		Input & (?, 250, 5) & 0 \\
		\hline
		Forward LSTM & (?, 256) & 268288 \\
		\hline
		Backward LSTM & (?, 256) & 268288 \\
		\hline
		Mu & (?, 128) & 65664 \\
		\hline
		Log Sigma & (?, 128) & 65664 \\
		\hline
		State initializer & (?, 1024) & 132096 \\
		\hline
		Decoder LSTM & (?, 512) & 1323008 \\
		\hline
		Output & (?, 123) & 63099 \\
		\hline
		\hline
		\multicolumn{3}{c}{Numero di variabili totale: 2186107} \\
		\hline
		\hline
	\end{tabular}
	\caption{I layer ottenuti attraverso la configurazione standard.}
	\label{tab:1}
\end{table}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{img/encoder_variables.png}
	\caption{I layer ottenuti attraverso la configurazione non condizionata.}
\end{figure}

\subsection{Generazione condizionale} % (fold)
\label{sub:generazione_condizionale}
\begin{figure}[ht]
	\centering
	\begin{tabular}{ccc}
		\includegraphics[width=0.2\linewidth]{img/cat_enc.png} &
		\includegraphics[width=0.2\linewidth]{img/cat_dec_0.png} &
		\includegraphics[width=0.2\linewidth]{img/cat_dec_1.png} \\
		\includegraphics[width=0.2\linewidth]{img/cat_dec_2.png} &
		\includegraphics[width=0.2\linewidth]{img/cat_dec_3.png} &
		\includegraphics[width=0.2\linewidth]{img/cat_dec_4.png} \\
		\includegraphics[width=0.2\linewidth]{img/cat_dec_5.png} &
		\includegraphics[width=0.2\linewidth]{img/cat_dec_6.png} &
		\includegraphics[width=0.2\linewidth]{img/cat_dec_7.png}
	\end{tabular}
	\caption{La prima immagine è estratta dal dataset, le altre sono immagini originali generate dal modello standard}
	\label{fig:1.17}
\end{figure}
% subsection generazione_condizionale (end)

\subsection{Generazione non condizionale} % (fold)
\label{sub:generazione_non_condizionale}
\begin{figure}[ht]
	\centering
	\begin{tabular}{c}
		\includegraphics[width=\linewidth]{img/owl_temp_02.png} \\
		\includegraphics[width=\linewidth]{img/owl_temp_04.png} \\
		\includegraphics[width=\linewidth]{img/owl_temp_05.png} \\
		\includegraphics[width=\linewidth]{img/owl_temp_06.png} \\
		\includegraphics[width=\linewidth]{img/owl_temp_08.png} \\
		\includegraphics[width=\linewidth]{img/owl_temp_1.png}
	\end{tabular}
	\caption{Immagini generate dal modello autoregressivo al variare del parametro di temperatura}
\end{figure}
% subsection generazione_non_condizionale (end)
% section esperimenti (end)

\section{training} % (fold)
\label{sec:training}
% section training (end)
\begin{minipage}{\linewidth}
\begin{lstlisting}[language = Python, frame = single, caption = {Implementazione in Keras del calcolo dell'errore di ricostruzione, suddiviso fra i termini dell'errore nell'offset $(L_s)$ e l'errore degli stati della penna $(L_p)$}, captionpos = b]
def get_lossfunc(out_pi, out_mu_x, out_mu_y, out_sigma_x, out_sigma_y, out_ro, out_q, x, y, logits):
    # L_r loss term calculation, L_s part
    result = tf_bi_normal(x, y, out_mu_x, out_mu_y, out_sigma_x, out_sigma_y, out_ro)
    result = result * out_pi
    result = K.sum(result, axis=1, keepdims=True)
    result = -K.log(result + 1e-8)
    fs = 1.0 - logits[:, 2]
    fs = K.reshape(fs, (-1, 1))
    result = result * fs
    # L_r loss term, L_p part
    result1 = K.categorical_crossentropy(out_q, logits, from_logits = True)
    result1 = K.reshape(result1, (-1, 1))
    result = result + result1
    return K.mean(result)
\end{lstlisting}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{lstlisting}[language = Python, frame = single, caption = {}, captionpos = b]

\end{lstlisting}
\end{minipage}