\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{sketchrnn}
\citation{quickdraw}
\citation{keras}
\citation{tensorflow}
\citation{GAN}
\citation{VI}
\citation{AR}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}\spacedlowsmallcaps  {Introduzione}}{7}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Prefazione}{7}{section.1.1}}
\newlabel{sec:prefazione}{{1.1}{7}{Prefazione}{section.1.1}{}}
\@writefile{brf}{\backcite{sketchrnn}{{7}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{quickdraw}{{7}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{keras}{{7}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{tensorflow}{{7}{1.1}{section.1.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Interpolazioni nello spazio di latenza di immagini vettoriali generate dal modello.\relax }}{7}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1.1}{{1}{7}{Interpolazioni nello spazio di latenza di immagini vettoriali generate dal modello.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Stato dell'arte}{7}{section.1.2}}
\mph@setcol{ii:7}{\mph@nr}
\@writefile{brf}{\backcite{GAN}{{8}{1.2}{section.1.2}}}
\@writefile{brf}{\backcite{VI}{{8}{1.2}{section.1.2}}}
\@writefile{brf}{\backcite{AR}{{8}{1.2}{section.1.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Reti neurali}{8}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Reti densamente connesse}{8}{section.1.4}}
\newlabel{sec:reti_densamente_connesse}{{1.4}{8}{Reti densamente connesse}{section.1.4}{}}
\mph@setcol{ii:8}{\mph@nr}
\citation{approx}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Rappresentazione di un neurone artificiale.\relax }}{9}{figure.caption.4}}
\newlabel{fig:1.2}{{2}{9}{Rappresentazione di un neurone artificiale.\relax }{figure.caption.4}{}}
\@writefile{brf}{\backcite{approx}{{9}{1.4}{figure.caption.5}}}
\mph@setcol{ii:9}{\mph@nr}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Rappresentazione di un modello Multi-Strato.\relax }}{10}{figure.caption.5}}
\newlabel{fig:1.3}{{3}{10}{Rappresentazione di un modello Multi-Strato.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Reti ricorrenti}{10}{section.1.5}}
\newlabel{sec:reti_ricorrenti}{{1.5}{10}{Reti ricorrenti}{section.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Una semplice RNN (Recurrent Neural Network).\relax }}{10}{figure.caption.6}}
\newlabel{fig:1.4}{{4}{10}{Una semplice RNN (Recurrent Neural Network).\relax }{figure.caption.6}{}}
\mph@setcol{ii:10}{\mph@nr}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Una RNN dispiegata lungo la linea temporale.\relax }}{11}{figure.caption.7}}
\newlabel{fig:1.5}{{5}{11}{Una RNN dispiegata lungo la linea temporale.\relax }{figure.caption.7}{}}
\mph@setcol{ii:11}{\mph@nr}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Due diversi metodi di implementazione della memoria in una RNN\relax }}{12}{figure.caption.8}}
\newlabel{fig:1.6}{{6}{12}{Due diversi metodi di implementazione della memoria in una RNN\relax }{figure.caption.8}{}}
\mph@setcol{ii:12}{\mph@nr}
\citation{rnn_effect}
\citation{rnn_effect}
\citation{vanishing}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Combinazioni di sequenze vettoriali. \cite  {rnn_effect}\relax }}{13}{figure.caption.9}}
\@writefile{brf}{\backcite{rnn_effect}{{13}{7}{figure.caption.9}}}
\newlabel{fig:1.7}{{7}{13}{Combinazioni di sequenze vettoriali. \cite {rnn_effect}\relax }{figure.caption.9}{}}
\newlabel{enum:recurrence}{{1.5}{13}{Reti ricorrenti}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Dipendenze a lungo termine}{13}{subsection.1.5.1}}
\newlabel{sub:dipendenze_a_lungo_termine}{{1.5.1}{13}{Dipendenze a lungo termine}{subsection.1.5.1}{}}
\mph@setcol{ii:13}{\mph@nr}
\citation{vanishing}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Struttura interna di una RNN standard con un singolo hidden layer.\relax }}{14}{figure.caption.10}}
\newlabel{fig:1.8}{{8}{14}{Struttura interna di una RNN standard con un singolo hidden layer.\relax }{figure.caption.10}{}}
\@writefile{brf}{\backcite{vanishing}{{14}{1.5.1}{figure.caption.10}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Long Short Term Memory}{14}{subsection.1.5.2}}
\newlabel{sub:lstm}{{1.5.2}{14}{Long Short Term Memory}{subsection.1.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Struttura interna di una lstm che evidenzia i quattro strati interni di un layer\relax }}{14}{figure.caption.11}}
\newlabel{fig:1.9}{{9}{14}{Struttura interna di una lstm che evidenzia i quattro strati interni di un layer\relax }{figure.caption.11}{}}
\@writefile{brf}{\backcite{vanishing}{{14}{1.5.2}{figure.caption.11}}}
\mph@setcol{ii:14}{\mph@nr}
\citation{peephole}
\citation{GRU}
\citation{DGRNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}L'informazione futura}{15}{subsection.1.5.3}}
\newlabel{sub:l_informazione_futura}{{1.5.3}{15}{L'informazione futura}{subsection.1.5.3}{}}
\@writefile{brf}{\backcite{peephole}{{15}{1}{figure.caption.11}}}
\@writefile{brf}{\backcite{GRU}{{15}{1}{figure.caption.11}}}
\@writefile{brf}{\backcite{DGRNN}{{15}{1}{figure.caption.11}}}
\mph@setcol{ii:15}{\mph@nr}
\citation{phone}
\citation{BRNN}
\citation{SDT}
\citation{combining}
\citation{phone1}
\@writefile{brf}{\backcite{phone}{{16}{1.5.3}{subsection.1.5.3}}}
\@writefile{brf}{\backcite{BRNN}{{16}{1.5.3}{subsection.1.5.3}}}
\@writefile{brf}{\backcite{SDT}{{16}{1.5.3}{subsection.1.5.3}}}
\@writefile{brf}{\backcite{combining}{{16}{1.5.3}{subsection.1.5.3}}}
\@writefile{brf}{\backcite{phone1}{{16}{1.5.3}{subsection.1.5.3}}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Confronto sull'utilizzo dell'input in diverse reti neurali.\relax }}{16}{figure.caption.12}}
\newlabel{fig:1.10}{{10}{16}{Confronto sull'utilizzo dell'input in diverse reti neurali.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.4}Reti bidirezionali}{16}{subsection.1.5.4}}
\newlabel{sub:reti_bidirezionali}{{1.5.4}{16}{Reti bidirezionali}{subsection.1.5.4}{}}
\mph@setcol{ii:16}{\mph@nr}
\citation{BRNN}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Struttura generica di una BRNN, svolta lungo tre time steps.\relax }}{17}{figure.caption.13}}
\newlabel{fig:1.11}{{11}{17}{Struttura generica di una BRNN, svolta lungo tre time steps.\relax }{figure.caption.13}{}}
\mph@setcol{ii:17}{\mph@nr}
\citation{keras_blog}
\citation{keras_blog}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Autoencoder}{18}{section.1.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Struttura di un generico AutoEncoder, da \cite  {keras_blog}\relax }}{18}{figure.caption.14}}
\@writefile{brf}{\backcite{keras_blog}{{18}{12}{figure.caption.14}}}
\newlabel{fig:1.12}{{12}{18}{Struttura di un generico AutoEncoder, da \cite {keras_blog}\relax }{figure.caption.14}{}}
\@writefile{brf}{\backcite{BRNN}{{18}{2}{figure.caption.13}}}
\mph@setcol{ii:18}{\mph@nr}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Modelli Generativi}{19}{section.1.7}}
\newlabel{sec:modelli_generativi}{{1.7}{19}{Modelli Generativi}{section.1.7}{}}
\newlabel{conditional}{{\relax 1.1}{19}{Modelli Generativi}{equation.1.7.1}{}}
\newlabel{joint}{{\relax 1.2}{19}{Modelli Generativi}{equation.1.7.2}{}}
\mph@setcol{ii:19}{\mph@nr}
\citation{VAE_tut}
\citation{VAE_tut}
\citation{VAE}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Variabili latenti}{20}{section.1.8}}
\newlabel{sec:variabili_latenti}{{1.8}{20}{Variabili latenti}{section.1.8}{}}
\newlabel{probability}{{\relax 1.3}{20}{Variabili latenti}{equation.1.8.3}{}}
\mph@setcol{ii:20}{\mph@nr}
\citation{rvg}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Modello di un VAE, da \cite  {VAE_tut}\relax }}{21}{figure.caption.15}}
\@writefile{brf}{\backcite{VAE_tut}{{21}{13}{figure.caption.15}}}
\newlabel{fig:1.13}{{13}{21}{Modello di un VAE, da \cite {VAE_tut}\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Variational Autoencoder}{21}{section.1.9}}
\newlabel{sec:vae}{{1.9}{21}{Variational Autoencoder}{section.1.9}{}}
\@writefile{brf}{\backcite{VAE}{{21}{1.9}{figure.caption.15}}}
\newlabel{gaussian_probability}{{\relax 1.4}{21}{Variational Autoencoder}{equation.1.9.4}{}}
\mph@setcol{ii:21}{\mph@nr}
\@writefile{brf}{\backcite{rvg}{{22}{3}{equation.1.9.4}}}
\mph@setcol{ii:22}{\mph@nr}
\newlabel{vae_loss}{{\relax 1.5}{23}{Variational Autoencoder}{equation.1.9.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Un VAE per intero, a sinistra senza il "reparametrization trick", a destra con. In rosso le operazioni non differenziabili, in blu le loss. Queste due varianti differiscono per il fatto che la retropropagazione si pu\IeC {\`o} applicare solo sulla rete a destra.\relax }}{23}{figure.caption.16}}
\newlabel{fig:1.14}{{14}{23}{Un VAE per intero, a sinistra senza il "reparametrization trick", a destra con. In rosso le operazioni non differenziabili, in blu le loss. Queste due varianti differiscono per il fatto che la retropropagazione si pu√≤ applicare solo sulla rete a destra.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.10}Problemi inversi}{23}{section.1.10}}
\newlabel{sec:problemi_inversi}{{1.10}{23}{Problemi inversi}{section.1.10}{}}
\mph@setcol{ii:23}{\mph@nr}
\citation{bfgs}
\newlabel{direct}{{\relax 1.6}{24}{Problemi inversi}{equation.1.10.6}{}}
\@writefile{brf}{\backcite{bfgs}{{24}{1.10}{equation.1.10.6}}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Un semplice esempio di problema diretto: sono evidenziati 1000 punti corrispondenti ai dati (i cerchi) generati dalla mappatura $x = t + 0.3 \qopname  \relax o{sin}(2 \pi t) + \epsilon $ dove $\epsilon $ \IeC {\`e} una variabile casuale. La curva rappresenta il risultato dell'addestramento di un MLP con cinque unit\IeC {\`a} e somma di quadrati come funzione d'errore. La rete approssima la media condizionale del target, che d\IeC {\`a} una buona rappresentazione del generatore dei dati.\relax }}{24}{figure.caption.17}}
\newlabel{fig:1.20}{{15}{24}{Un semplice esempio di problema diretto: sono evidenziati 1000 punti corrispondenti ai dati (i cerchi) generati dalla mappatura $x = t + 0.3 \sin (2 \pi t) + \epsilon $ dove $\epsilon $ √® una variabile casuale. La curva rappresenta il risultato dell'addestramento di un MLP con cinque unit√† e somma di quadrati come funzione d'errore. La rete approssima la media condizionale del target, che d√† una buona rappresentazione del generatore dei dati.\relax }{figure.caption.17}{}}
\mph@setcol{ii:24}{\mph@nr}
\citation{gmm}
\citation{mixture}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Quest'immagine mostra esattamente lo stesso dataset della figura \ref  {fig:1.20}, invertendo input e variabili target. La curva mostra nuovamente il risultato dell'addestramento di una MLP con funzione \textit  {sum-of-squares}. Stavolta la rete ottiene un pessimo risultato, continuando a tentare di rappresentare la media condizionale.\relax }}{25}{figure.caption.18}}
\newlabel{fig:1.21}{{16}{25}{Quest'immagine mostra esattamente lo stesso dataset della figura \ref {fig:1.20}, invertendo input e variabili target. La curva mostra nuovamente il risultato dell'addestramento di una MLP con funzione \textit {sum-of-squares}. Stavolta la rete ottiene un pessimo risultato, continuando a tentare di rappresentare la media condizionale.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.11}Mixture Density Network}{25}{section.1.11}}
\newlabel{sec:mdn}{{1.11}{25}{Mixture Density Network}{section.1.11}{}}
\@writefile{brf}{\backcite{gmm}{{25}{1.11}{section.1.11}}}
\@writefile{brf}{\backcite{mixture}{{25}{1.11}{section.1.11}}}
\mph@setcol{ii:25}{\mph@nr}
\newlabel{density}{{\relax 1.7}{26}{Mixture Density Network}{equation.1.11.7}{}}
\newlabel{kernel}{{\relax 1.8}{26}{Mixture Density Network}{equation.1.11.8}{}}
\mph@setcol{ii:26}{\mph@nr}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Rappresentazione di una MDN. L'output della rete neurale determina i parametri di una mistura. Di conseguenza, il modello rappresenta la funzione di densit\IeC {\`a} di probabilit\IeC {\`a} condizionale delle variabili target condizionate all'input $\boldsymbol  {x}$ della rete.\relax }}{27}{figure.caption.19}}
\newlabel{fig:1.22}{{17}{27}{Rappresentazione di una MDN. L'output della rete neurale determina i parametri di una mistura. Di conseguenza, il modello rappresenta la funzione di densit√† di probabilit√† condizionale delle variabili target condizionate all'input $\boldsymbol {x}$ della rete.\relax }{figure.caption.19}{}}
\newlabel{softmax}{{\relax 1.9}{27}{Mixture Density Network}{equation.1.11.9}{}}
\newlabel{exponential}{{\relax 1.10}{27}{Mixture Density Network}{equation.1.11.10}{}}
\mph@setcol{ii:27}{\mph@nr}
\newlabel{mus}{{\relax 1.11}{28}{Mixture Density Network}{equation.1.11.11}{}}
\newlabel{mdnerror}{{\relax 1.12}{28}{Mixture Density Network}{equation.1.11.12}{}}
\mph@setcol{ii:28}{\mph@nr}
\@setckpt{Chapters/introduzione}{
\setcounter{page}{29}
\setcounter{equation}{12}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{5}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{11}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{17}
\setcounter{table}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{dummy}{0}
\setcounter{Item}{0}
\setcounter{bookmark@seq@number}{16}
\setcounter{lstlisting}{0}
\setcounter{section@level}{0}
}
