\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{sketchrnn}
\citation{quickdraw}
\citation{keras}
\citation{tensorflow}
\citation{GAN}
\citation{VI}
\citation{AR}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}\spacedlowsmallcaps  {Introduzione}}{9}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Prefazione}{9}{section.1.1}}
\newlabel{sec:prefazione}{{1.1}{9}{Prefazione}{section.1.1}{}}
\@writefile{brf}{\backcite{sketchrnn}{{9}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{quickdraw}{{9}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{keras}{{9}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{tensorflow}{{9}{1.1}{section.1.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Interpolazioni nello spazio di latenza di immagini vettoriali generate dal modello.\relax }}{9}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1.1}{{1}{9}{Interpolazioni nello spazio di latenza di immagini vettoriali generate dal modello.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Stato dell'arte}{9}{section.1.2}}
\mph@setcol{ii:9}{\mph@nr}
\@writefile{brf}{\backcite{GAN}{{10}{1.2}{section.1.2}}}
\@writefile{brf}{\backcite{VI}{{10}{1.2}{section.1.2}}}
\@writefile{brf}{\backcite{AR}{{10}{1.2}{section.1.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Reti neurali}{10}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Reti densamente connesse}{10}{section.1.4}}
\newlabel{sec:reti_densamente_connesse}{{1.4}{10}{Reti densamente connesse}{section.1.4}{}}
\mph@setcol{ii:10}{\mph@nr}
\citation{approx}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Rappresentazione di un neurone artificiale.\relax }}{11}{figure.caption.4}}
\newlabel{fig:1.2}{{2}{11}{Rappresentazione di un neurone artificiale.\relax }{figure.caption.4}{}}
\@writefile{brf}{\backcite{approx}{{11}{1.4}{figure.caption.5}}}
\mph@setcol{ii:11}{\mph@nr}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Rappresentazione di un modello Multi-Strato.\relax }}{12}{figure.caption.5}}
\newlabel{fig:1.3}{{3}{12}{Rappresentazione di un modello Multi-Strato.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Reti ricorrenti}{12}{section.1.5}}
\newlabel{sec:reti_ricorrenti}{{1.5}{12}{Reti ricorrenti}{section.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Una semplice RNN (Recurrent Neural Network).\relax }}{12}{figure.caption.6}}
\newlabel{fig:1.4}{{4}{12}{Una semplice RNN (Recurrent Neural Network).\relax }{figure.caption.6}{}}
\mph@setcol{ii:12}{\mph@nr}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Una RNN dispiegata lungo la linea temporale.\relax }}{13}{figure.caption.7}}
\newlabel{fig:1.5}{{5}{13}{Una RNN dispiegata lungo la linea temporale.\relax }{figure.caption.7}{}}
\mph@setcol{ii:13}{\mph@nr}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Due diversi metodi di implementazione della memoria in una RNN\relax }}{14}{figure.caption.8}}
\newlabel{fig:1.6}{{6}{14}{Due diversi metodi di implementazione della memoria in una RNN\relax }{figure.caption.8}{}}
\mph@setcol{ii:14}{\mph@nr}
\citation{rnn_effect}
\citation{rnn_effect}
\citation{vanishing}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Combinazioni di sequenze vettoriali. \cite  {rnn_effect}\relax }}{15}{figure.caption.9}}
\@writefile{brf}{\backcite{rnn_effect}{{15}{7}{figure.caption.9}}}
\newlabel{fig:1.7}{{7}{15}{Combinazioni di sequenze vettoriali. \cite {rnn_effect}\relax }{figure.caption.9}{}}
\newlabel{enum:recurrence}{{1.5}{15}{Reti ricorrenti}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Dipendenze a lungo termine}{15}{subsection.1.5.1}}
\newlabel{sub:dipendenze_a_lungo_termine}{{1.5.1}{15}{Dipendenze a lungo termine}{subsection.1.5.1}{}}
\mph@setcol{ii:15}{\mph@nr}
\citation{LSTM}
\citation{LSTM}
\citation{vanishing}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Struttura interna di una RNN standard con un singolo hidden layer.\relax }}{16}{figure.caption.10}}
\newlabel{fig:1.8}{{8}{16}{Struttura interna di una RNN standard con un singolo hidden layer.\relax }{figure.caption.10}{}}
\@writefile{brf}{\backcite{vanishing}{{16}{1.5.1}{figure.caption.10}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Long Short Term Memory}{16}{subsection.1.5.2}}
\newlabel{sub:lstm}{{1.5.2}{16}{Long Short Term Memory}{subsection.1.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Struttura interna di una lstm che evidenzia i quattro strati interni di un layer. Immagine da \cite  {LSTM}\relax }}{16}{figure.caption.11}}
\@writefile{brf}{\backcite{LSTM}{{16}{9}{figure.caption.11}}}
\newlabel{fig:1.9}{{9}{16}{Struttura interna di una lstm che evidenzia i quattro strati interni di un layer. Immagine da \cite {LSTM}\relax }{figure.caption.11}{}}
\@writefile{brf}{\backcite{vanishing}{{16}{1.5.2}{figure.caption.11}}}
\mph@setcol{ii:16}{\mph@nr}
\citation{peephole}
\citation{GRU}
\citation{DGRNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}L'informazione futura}{17}{subsection.1.5.3}}
\newlabel{sub:l_informazione_futura}{{1.5.3}{17}{L'informazione futura}{subsection.1.5.3}{}}
\@writefile{brf}{\backcite{peephole}{{17}{1}{figure.caption.11}}}
\@writefile{brf}{\backcite{GRU}{{17}{1}{figure.caption.11}}}
\@writefile{brf}{\backcite{DGRNN}{{17}{1}{figure.caption.11}}}
\mph@setcol{ii:17}{\mph@nr}
\citation{phone}
\citation{BRNN}
\citation{SDT}
\citation{combining}
\citation{phone1}
\@writefile{brf}{\backcite{phone}{{18}{1.5.3}{subsection.1.5.3}}}
\@writefile{brf}{\backcite{BRNN}{{18}{1.5.3}{subsection.1.5.3}}}
\@writefile{brf}{\backcite{SDT}{{18}{1.5.3}{subsection.1.5.3}}}
\@writefile{brf}{\backcite{combining}{{18}{1.5.3}{subsection.1.5.3}}}
\@writefile{brf}{\backcite{phone1}{{18}{1.5.3}{subsection.1.5.3}}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Confronto sull'utilizzo dell'input in diverse reti neurali.\relax }}{18}{figure.caption.12}}
\newlabel{fig:1.10}{{10}{18}{Confronto sull'utilizzo dell'input in diverse reti neurali.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.4}Reti bidirezionali}{18}{subsection.1.5.4}}
\newlabel{sub:reti_bidirezionali}{{1.5.4}{18}{Reti bidirezionali}{subsection.1.5.4}{}}
\mph@setcol{ii:18}{\mph@nr}
\citation{BRNN}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Struttura generica di una BRNN, svolta lungo tre time steps.\relax }}{19}{figure.caption.13}}
\newlabel{fig:1.11}{{11}{19}{Struttura generica di una BRNN, svolta lungo tre time steps.\relax }{figure.caption.13}{}}
\mph@setcol{ii:19}{\mph@nr}
\citation{keras_blog}
\citation{keras_blog}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Autoencoder}{20}{section.1.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Struttura di un generico AutoEncoder, da \cite  {keras_blog}\relax }}{20}{figure.caption.14}}
\@writefile{brf}{\backcite{keras_blog}{{20}{12}{figure.caption.14}}}
\newlabel{fig:1.12}{{12}{20}{Struttura di un generico AutoEncoder, da \cite {keras_blog}\relax }{figure.caption.14}{}}
\@writefile{brf}{\backcite{BRNN}{{20}{2}{figure.caption.13}}}
\mph@setcol{ii:20}{\mph@nr}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Modelli Generativi}{21}{section.1.7}}
\newlabel{sec:modelli_generativi}{{1.7}{21}{Modelli Generativi}{section.1.7}{}}
\newlabel{conditional}{{\relax 1.1}{21}{Modelli Generativi}{equation.1.7.1}{}}
\newlabel{joint}{{\relax 1.2}{21}{Modelli Generativi}{equation.1.7.2}{}}
\mph@setcol{ii:21}{\mph@nr}
\citation{VAE_tut}
\citation{VAE_tut}
\citation{VAE}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Variabili latenti}{22}{section.1.8}}
\newlabel{sec:variabili_latenti}{{1.8}{22}{Variabili latenti}{section.1.8}{}}
\newlabel{probability}{{\relax 1.3}{22}{Variabili latenti}{equation.1.8.3}{}}
\mph@setcol{ii:22}{\mph@nr}
\citation{rvg}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Modello di un VAE, da \cite  {VAE_tut}\relax }}{23}{figure.caption.15}}
\@writefile{brf}{\backcite{VAE_tut}{{23}{13}{figure.caption.15}}}
\newlabel{fig:1.13}{{13}{23}{Modello di un VAE, da \cite {VAE_tut}\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Variational Autoencoder}{23}{section.1.9}}
\newlabel{sec:vae}{{1.9}{23}{Variational Autoencoder}{section.1.9}{}}
\@writefile{brf}{\backcite{VAE}{{23}{1.9}{figure.caption.15}}}
\newlabel{gaussian_probability}{{\relax 1.4}{23}{Variational Autoencoder}{equation.1.9.4}{}}
\mph@setcol{ii:23}{\mph@nr}
\@writefile{brf}{\backcite{rvg}{{24}{3}{equation.1.9.4}}}
\mph@setcol{ii:24}{\mph@nr}
\newlabel{vae_loss}{{\relax 1.5}{25}{Variational Autoencoder}{equation.1.9.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Un VAE per intero, a sinistra senza il "reparametrization trick", a destra con. In rosso le operazioni non differenziabili, in blu le loss. Queste due varianti differiscono per il fatto che la retropropagazione si pu\IeC {\`o} applicare solo sulla rete a destra.\relax }}{25}{figure.caption.16}}
\newlabel{fig:1.14}{{14}{25}{Un VAE per intero, a sinistra senza il "reparametrization trick", a destra con. In rosso le operazioni non differenziabili, in blu le loss. Queste due varianti differiscono per il fatto che la retropropagazione si pu√≤ applicare solo sulla rete a destra.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.10}Problemi inversi}{25}{section.1.10}}
\newlabel{sec:problemi_inversi}{{1.10}{25}{Problemi inversi}{section.1.10}{}}
\mph@setcol{ii:25}{\mph@nr}
\citation{bfgs}
\newlabel{direct}{{\relax 1.6}{26}{Problemi inversi}{equation.1.10.6}{}}
\@writefile{brf}{\backcite{bfgs}{{26}{1.10}{equation.1.10.6}}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Un semplice esempio di problema diretto: sono evidenziati 1000 punti corrispondenti ai dati (i cerchi) generati dalla mappatura $x = t + 0.3 \qopname  \relax o{sin}(2 \pi t) + \epsilon $ dove $\epsilon $ \IeC {\`e} una variabile casuale. La curva rappresenta il risultato dell'addestramento di un MLP con cinque unit\IeC {\`a} e somma di quadrati come funzione d'errore. La rete approssima la media condizionale del target, che d\IeC {\`a} una buona rappresentazione del generatore dei dati.\relax }}{26}{figure.caption.17}}
\newlabel{fig:1.20}{{15}{26}{Un semplice esempio di problema diretto: sono evidenziati 1000 punti corrispondenti ai dati (i cerchi) generati dalla mappatura $x = t + 0.3 \sin (2 \pi t) + \epsilon $ dove $\epsilon $ √® una variabile casuale. La curva rappresenta il risultato dell'addestramento di un MLP con cinque unit√† e somma di quadrati come funzione d'errore. La rete approssima la media condizionale del target, che d√† una buona rappresentazione del generatore dei dati.\relax }{figure.caption.17}{}}
\mph@setcol{ii:26}{\mph@nr}
\citation{gmm}
\citation{mixture}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Quest'immagine mostra esattamente lo stesso dataset della figura \ref  {fig:1.20}, invertendo input e variabili target. La curva mostra nuovamente il risultato dell'addestramento di una MLP con funzione \textit  {sum-of-squares}. Stavolta la rete ottiene un pessimo risultato, continuando a tentare di rappresentare la media condizionale.\relax }}{27}{figure.caption.18}}
\newlabel{fig:1.21}{{16}{27}{Quest'immagine mostra esattamente lo stesso dataset della figura \ref {fig:1.20}, invertendo input e variabili target. La curva mostra nuovamente il risultato dell'addestramento di una MLP con funzione \textit {sum-of-squares}. Stavolta la rete ottiene un pessimo risultato, continuando a tentare di rappresentare la media condizionale.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.11}Mixture Density Network}{27}{section.1.11}}
\newlabel{sec:mdn}{{1.11}{27}{Mixture Density Network}{section.1.11}{}}
\@writefile{brf}{\backcite{gmm}{{27}{1.11}{section.1.11}}}
\@writefile{brf}{\backcite{mixture}{{27}{1.11}{section.1.11}}}
\mph@setcol{ii:27}{\mph@nr}
\citation{MDN}
\citation{MDN}
\newlabel{density}{{\relax 1.7}{28}{Mixture Density Network}{equation.1.11.7}{}}
\newlabel{kernel}{{\relax 1.8}{28}{Mixture Density Network}{equation.1.11.8}{}}
\mph@setcol{ii:28}{\mph@nr}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Rappresentazione di una MDN. L'output della rete neurale determina i parametri di una mistura. Di conseguenza, il modello rappresenta la funzione di densit\IeC {\`a} di probabilit\IeC {\`a} condizionale delle variabili target condizionate all'input $\boldsymbol  {x}$ della rete. Immagine da \cite  {MDN}\relax }}{29}{figure.caption.19}}
\@writefile{brf}{\backcite{MDN}{{29}{17}{figure.caption.19}}}
\newlabel{fig:1.22}{{17}{29}{Rappresentazione di una MDN. L'output della rete neurale determina i parametri di una mistura. Di conseguenza, il modello rappresenta la funzione di densit√† di probabilit√† condizionale delle variabili target condizionate all'input $\boldsymbol {x}$ della rete. Immagine da \cite {MDN}\relax }{figure.caption.19}{}}
\newlabel{softmax}{{\relax 1.9}{29}{Mixture Density Network}{equation.1.11.9}{}}
\newlabel{exponential}{{\relax 1.10}{29}{Mixture Density Network}{equation.1.11.10}{}}
\mph@setcol{ii:29}{\mph@nr}
\newlabel{mus}{{\relax 1.11}{30}{Mixture Density Network}{equation.1.11.11}{}}
\newlabel{mdnerror}{{\relax 1.12}{30}{Mixture Density Network}{equation.1.11.12}{}}
\mph@setcol{ii:30}{\mph@nr}
\@setckpt{Chapters/introduzione}{
\setcounter{page}{31}
\setcounter{equation}{12}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{5}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{11}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{17}
\setcounter{table}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{dummy}{0}
\setcounter{Item}{0}
\setcounter{bookmark@seq@number}{16}
\setcounter{lstlisting}{0}
\setcounter{section@level}{0}
}
